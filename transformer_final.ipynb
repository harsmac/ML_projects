{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <!--\n",
    " Author : Harshitha Machiraju\n",
    " Date: 20/09/2024\n",
    "   -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Author** : Harshitha Machiraju\n",
    "\n",
    " **Date**: 20/09/2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import dataset\n",
    "from torchtext.datasets import PennTreebank\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from unittest import TestCase\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention module.\n",
    "\n",
    "    This module implements the Multi-Head Attention mechanism as described in \n",
    "    \"Attention Is All You Need\" (Vaswani et al., 2017). It projects the input into\n",
    "    multiple heads, applies scaled dot-product attention, and then concatenates\n",
    "    the results.\n",
    "\n",
    "    Args:\n",
    "        dim (int): The input and output dimension of the model.\n",
    "        n_heads (int): The number of attention heads.\n",
    "        dropout (Optional[float]): Dropout probability. Defaults to None (no dropout).\n",
    "\n",
    "    Attributes:\n",
    "        dim (int): The input and output dimension.\n",
    "        n_heads (int): The number of attention heads.\n",
    "        head_dim (int): The dimension of each attention head.\n",
    "        dropout (nn.Dropout): Dropout layer.\n",
    "        linear_query (nn.Linear): Linear projection for query.\n",
    "        linear_key (nn.Linear): Linear projection for key.\n",
    "        linear_value (nn.Linear): Linear projection for value.\n",
    "        linear_cat_attn (nn.Linear): Final linear projection after attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, n_heads: int, dropout: Optional[float] = None):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = dim // n_heads\n",
    "        assert n_heads * self.head_dim == dim, f\"embedding dim={dim} not divisible by n_heads={n_heads}.\"\n",
    "        self.dropout = nn.Dropout(p=dropout if dropout is not None else 0.0)\n",
    "        self.linear_query = nn.Linear(dim, dim)\n",
    "        self.linear_key = nn.Linear(dim, dim)\n",
    "        self.linear_key = nn.Linear(dim, dim)\n",
    "        self.linear_cat_attn = nn.Linear(dim, dim)\n",
    "        # self.max_positions = 512\n",
    "        # self.register_buffer(\"tril\", torch.tril(torch.ones(1, 1, self.max_positions, self.max_positions)))\n",
    "        # self.T = 0\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Applies multi-head attention to input tensors.\n",
    "\n",
    "        Args:\n",
    "            q (Tensor): Query tensor of shape (batch_size, seq_len, dim).\n",
    "            k (Tensor): Key tensor of shape (batch_size, seq_len, dim).\n",
    "            v (Tensor): Value tensor of shape (batch_size, seq_len, dim).\n",
    "            mask (Optional[Tensor]): Attention mask tensor. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying multi-head attention.\n",
    "        \"\"\"\n",
    "\n",
    "        # Linear projections\n",
    "        batch_size, T, _ = q.size()\n",
    "        self.T = T\n",
    "        q = self.linear_query(q).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.linear_key(k).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.linear_key(v).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention\n",
    "        if mask is not None:\n",
    "            # print(mask.shape)\n",
    "            mask = mask.repeat(self.n_heads, 1, 1)\n",
    "            # mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        attn_output = self.attention(q, k, v, mask)\n",
    "        \n",
    "        # Concatenate heads and apply final linear projection\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.dim)\n",
    "\n",
    "        return self.linear_cat_attn(attn_output)\n",
    "\n",
    "    def attention(self, query: Tensor, key: Tensor, value: Tensor, mask: Optional[Tensor] = None) -> Tensor:\n",
    "        d_k = query.size()[-1]\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        # Below line not needed due to the mask shape not matching the scores shape !!\n",
    "        # if mask is not None:\n",
    "            # scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "            # scores = scores.masked_fill(self.tril[:,:,:self.T,:self.T] == 0, float('-inf')) # this works\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        return torch.matmul(attn, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, attn, d_model: int, dim_feedforward: int, layer_norm_eps=1e-5, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Class for the Encoder block of the transformer model.\n",
    "        :param attn: multi-head self-attention layer\n",
    "        :param d_model: hidden dimension of the input tensor\n",
    "        :param dim_feedforward: hidden dimension of the feedforward network\n",
    "        :param layer_norm_eps: epsilon for layer normalization\n",
    "        :param dropout: dropout rate\n",
    "        \n",
    "        \"\"\"\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attn = MultiHeadSelfAttention(d_model, d_model, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        self.linear_model = nn.Sequential(\n",
    "            self.linear1,\n",
    "            self.dropout,\n",
    "            self.activation,\n",
    "            self.linear2,\n",
    "            self.dropout\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the EncoderBlock.\n",
    "        :param x: input tensor\n",
    "        :param mask: attention mask\n",
    "        :return: output tensor of the encoder block\n",
    "        \"\"\"\n",
    "        attn_x = self._self_attn(x, mask)\n",
    "        x = self.norm1(x + attn_x)\n",
    "\n",
    "        ff_x = self._feed_forward(x)\n",
    "        return self.norm2(ff_x)\n",
    "\n",
    "    def _self_attn(self, x: Tensor, mask: Optional[Tensor]) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply self-attention to the input tensor.\n",
    "\n",
    "        :param x: input tensor\n",
    "        :param mask: attention mask\n",
    "        :return: tensor after applying self-attention and dropout\n",
    "        \"\"\"\n",
    "        x = self.attn(x, x, x, mask)\n",
    "        return self.dropout(x)\n",
    "\n",
    "    def _feed_forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply feed-forward network to the input tensor.\n",
    "\n",
    "        :param x: input tensor\n",
    "        :return: tensor after applying feed-forward network and dropout\n",
    "        \"\"\"\n",
    "        x = self.dropout(self.activation(self.linear1(x)))\n",
    "        return self.dropout(self.linear2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder: EncoderBlock, n_blocks: int):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.encoder_blocks = nn.ModuleList([deepcopy(encoder) for _ in range(n_blocks)])\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Optional[Tensor] = None) -> Tensor:\n",
    "        for encoder in self.encoder_blocks:\n",
    "            x = encoder(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "    Unmasked positions are filled with float(0.0).\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The code below is given as-is, please contact the examinator if there is any issue running this, unrelated to your code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncodingTorch(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding module for the Transformer model.\n",
    "\n",
    "    This module adds positional encoding to the input tensor. The positional encoding\n",
    "    is learned as a part of the model.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The input and output dimension of the model.\n",
    "        dropout (float): Dropout probability. Defaults to 0.1.\n",
    "        max_len (int): Maximum length of the input sequence. Defaults to 5000.\n",
    "\n",
    "    Attributes:\n",
    "        dropout (nn.Dropout): Dropout layer.\n",
    "        pe (Tensor): Positional encoding tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModelManualAttn(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer model for language modeling. With EncodingBlock, positional encoding and Attention implemented manually.\n",
    "    Args:\n",
    "        emsize (int): The embedding dimension.\n",
    "        ntoken (int): The size of the vocabulary.\n",
    "        d_model (int): The hidden dimension.\n",
    "        nhead (int): The number of attention heads.\n",
    "        d_hid (int): The hidden dimension of the feedforward network.\n",
    "        nlayers (int): The number of encoder layers.\n",
    "        dropout (float): Dropout probability. Defaults to 0.5.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emsize:int, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncodingTorch(d_model, dropout)\n",
    "        encoder_layers = EncoderBlock(attn=MultiHeadSelfAttention(dim=emsize, n_heads=nhead, dropout=dropout),\n",
    "                                      d_model=d_model, dim_feedforward=d_hid, dropout=dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Perform forward pass through the transformer model.\n",
    "        \n",
    "        :param src: Source tensor\n",
    "        :param src_mask: Source mask tensor\n",
    "        :return: Model output\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            # print(\"Generating mask\", src.size(0))\n",
    "            src_mask = generate_square_subsequent_mask(src.size(0)).to(src.device)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter: dataset.IterableDataset, vocab, tokenizer) -> Tensor:\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "\n",
    "def batchify(data: Tensor, bsz: int, device: torch.device = None) -> Tensor:\n",
    "    seq_len = data.shape[0] // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "\n",
    "def get_batch(source: Tensor, i: int, bptt: int) -> Tuple[Tensor, Tensor]:\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i + seq_len]\n",
    "    target = source[i + 1:i + 1 + seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model,\n",
    "        train_data: Tensor,\n",
    "        bptt: int,\n",
    "        criterion,\n",
    "        ntokens: int,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: torch.optim.lr_scheduler,\n",
    "        epoch: int = 0,\n",
    "        device: torch.device = None,\n",
    "        use_causal_mask: bool = True\n",
    ") -> None:\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    log_interval = 1\n",
    "    start_time = time.time()\n",
    "    src_mask = None  # We'll generate the mask for each batch\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.shape[0] - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i, bptt=bptt)\n",
    "        # print(f\"Batch {batch}:, data shape: {data.shape}, target shape: {targets.shape}\")\n",
    "        if use_causal_mask:\n",
    "            src_mask = generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        \n",
    "        # print(f\"Batch {batch}:\")\n",
    "        # print(f\"  Data shape: {data.shape}\")\n",
    "        # print(f\"  Mask shape: {src_mask.shape if src_mask is not None else 'None'}\")\n",
    "        \n",
    "        output = model(src=data, src_mask=src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f}'\n",
    "                  f' | ppl {ppl:8.2f}'\n",
    "                  )\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "        model,\n",
    "        eval_data: Tensor,\n",
    "        bptt: int,\n",
    "        ntokens: int,\n",
    "        criterion,\n",
    "        device: torch.device = None,\n",
    "        use_causal_mask: bool = True,\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device) if use_causal_mask else None\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.shape[0] - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i, bptt=bptt)\n",
    "            if data.shape[0] < bptt and src_mask is not None:\n",
    "                src_mask = generate_square_subsequent_mask(data.shape[0]).to(device)\n",
    "            # print(data, src_mask)\n",
    "            output = model(data, src_mask=src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += data.shape[0] * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     1/ 1320 batches | lr 5.00 | ms/batch 453.86 | loss 18.08 | ppl 71114753.31\n",
      "| epoch   1 |     2/ 1320 batches | lr 5.00 | ms/batch 147.39 | loss  9.28 | ppl 10755.84\n",
      "| epoch   1 |     3/ 1320 batches | lr 5.00 | ms/batch 159.54 | loss  9.74 | ppl 16946.52\n",
      "| epoch   1 |     4/ 1320 batches | lr 5.00 | ms/batch 209.50 | loss 11.02 | ppl 60844.06\n",
      "| epoch   1 |     5/ 1320 batches | lr 5.00 | ms/batch 160.96 | loss 10.67 | ppl 42830.99\n",
      "| epoch   1 |     6/ 1320 batches | lr 5.00 | ms/batch 154.59 | loss  9.65 | ppl 15476.97\n",
      "| epoch   1 |     7/ 1320 batches | lr 5.00 | ms/batch 149.53 | loss  9.09 | ppl  8887.89\n",
      "| epoch   1 |     8/ 1320 batches | lr 5.00 | ms/batch 148.10 | loss  9.55 | ppl 13978.37\n",
      "| epoch   1 |     9/ 1320 batches | lr 5.00 | ms/batch 150.11 | loss  9.33 | ppl 11217.22\n",
      "| epoch   1 |    10/ 1320 batches | lr 5.00 | ms/batch 150.93 | loss  9.88 | ppl 19500.56\n",
      "| epoch   1 |    11/ 1320 batches | lr 5.00 | ms/batch 149.12 | loss  9.50 | ppl 13415.97\n",
      "| epoch   1 |    12/ 1320 batches | lr 5.00 | ms/batch 149.63 | loss  7.94 | ppl  2801.52\n",
      "| epoch   1 |    13/ 1320 batches | lr 5.00 | ms/batch 147.33 | loss 11.31 | ppl 81642.66\n",
      "| epoch   1 |    14/ 1320 batches | lr 5.00 | ms/batch 152.55 | loss  8.11 | ppl  3341.91\n",
      "| epoch   1 |    15/ 1320 batches | lr 5.00 | ms/batch 153.17 | loss  8.58 | ppl  5318.63\n",
      "| epoch   1 |    16/ 1320 batches | lr 5.00 | ms/batch 170.21 | loss  9.01 | ppl  8210.09\n",
      "| epoch   1 |    17/ 1320 batches | lr 5.00 | ms/batch 147.24 | loss  7.85 | ppl  2554.20\n",
      "| epoch   1 |    18/ 1320 batches | lr 5.00 | ms/batch 149.69 | loss 11.40 | ppl 89760.44\n",
      "| epoch   1 |    19/ 1320 batches | lr 5.00 | ms/batch 146.94 | loss  8.13 | ppl  3384.92\n",
      "| epoch   1 |    20/ 1320 batches | lr 5.00 | ms/batch 149.40 | loss  8.29 | ppl  3982.68\n",
      "| epoch   1 |    21/ 1320 batches | lr 5.00 | ms/batch 148.03 | loss  9.43 | ppl 12506.67\n",
      "| epoch   1 |    22/ 1320 batches | lr 5.00 | ms/batch 155.95 | loss  8.10 | ppl  3295.96\n",
      "| epoch   1 |    23/ 1320 batches | lr 5.00 | ms/batch 147.13 | loss  7.62 | ppl  2048.68\n",
      "| epoch   1 |    24/ 1320 batches | lr 5.00 | ms/batch 154.64 | loss  7.57 | ppl  1939.51\n",
      "| epoch   1 |    25/ 1320 batches | lr 5.00 | ms/batch 147.82 | loss  7.65 | ppl  2093.25\n",
      "| epoch   1 |    26/ 1320 batches | lr 5.00 | ms/batch 161.43 | loss  7.73 | ppl  2285.75\n",
      "| epoch   1 |    27/ 1320 batches | lr 5.00 | ms/batch 146.80 | loss  8.29 | ppl  3980.07\n",
      "| epoch   1 |    28/ 1320 batches | lr 5.00 | ms/batch 149.62 | loss  8.14 | ppl  3433.50\n",
      "| epoch   1 |    29/ 1320 batches | lr 5.00 | ms/batch 145.82 | loss  8.48 | ppl  4796.82\n",
      "| epoch   1 |    30/ 1320 batches | lr 5.00 | ms/batch 148.41 | loss  7.86 | ppl  2596.21\n",
      "| epoch   1 |    31/ 1320 batches | lr 5.00 | ms/batch 145.87 | loss  7.51 | ppl  1818.71\n",
      "| epoch   1 |    32/ 1320 batches | lr 5.00 | ms/batch 148.20 | loss  7.40 | ppl  1630.38\n",
      "| epoch   1 |    33/ 1320 batches | lr 5.00 | ms/batch 145.43 | loss  8.02 | ppl  3035.08\n",
      "| epoch   1 |    34/ 1320 batches | lr 5.00 | ms/batch 165.41 | loss  8.55 | ppl  5168.22\n",
      "| epoch   1 |    35/ 1320 batches | lr 5.00 | ms/batch 149.32 | loss  7.63 | ppl  2061.86\n",
      "| epoch   1 |    36/ 1320 batches | lr 5.00 | ms/batch 147.65 | loss  7.49 | ppl  1797.52\n",
      "| epoch   1 |    37/ 1320 batches | lr 5.00 | ms/batch 145.77 | loss  9.34 | ppl 11336.66\n",
      "| epoch   1 |    38/ 1320 batches | lr 5.00 | ms/batch 151.02 | loss  7.87 | ppl  2627.11\n",
      "| epoch   1 |    39/ 1320 batches | lr 5.00 | ms/batch 147.43 | loss  7.97 | ppl  2898.70\n",
      "| epoch   1 |    40/ 1320 batches | lr 5.00 | ms/batch 150.16 | loss  7.64 | ppl  2078.92\n",
      "| epoch   1 |    41/ 1320 batches | lr 5.00 | ms/batch 147.92 | loss  7.38 | ppl  1606.81\n",
      "| epoch   1 |    42/ 1320 batches | lr 5.00 | ms/batch 157.01 | loss  7.95 | ppl  2842.05\n",
      "| epoch   1 |    43/ 1320 batches | lr 5.00 | ms/batch 146.81 | loss  7.66 | ppl  2131.13\n",
      "| epoch   1 |    44/ 1320 batches | lr 5.00 | ms/batch 148.95 | loss  7.82 | ppl  2479.88\n",
      "| epoch   1 |    45/ 1320 batches | lr 5.00 | ms/batch 147.64 | loss  7.63 | ppl  2049.20\n",
      "| epoch   1 |    46/ 1320 batches | lr 5.00 | ms/batch 147.73 | loss  7.24 | ppl  1395.35\n",
      "| epoch   1 |    47/ 1320 batches | lr 5.00 | ms/batch 147.82 | loss  8.65 | ppl  5711.82\n",
      "| epoch   1 |    48/ 1320 batches | lr 5.00 | ms/batch 152.78 | loss  7.37 | ppl  1587.08\n",
      "| epoch   1 |    49/ 1320 batches | lr 5.00 | ms/batch 151.16 | loss  7.71 | ppl  2234.76\n",
      "| epoch   1 |    50/ 1320 batches | lr 5.00 | ms/batch 147.35 | loss  7.52 | ppl  1845.89\n",
      "| epoch   1 |    51/ 1320 batches | lr 5.00 | ms/batch 145.39 | loss  7.20 | ppl  1340.30\n",
      "| epoch   1 |    52/ 1320 batches | lr 5.00 | ms/batch 149.34 | loss  9.89 | ppl 19643.67\n",
      "| epoch   1 |    53/ 1320 batches | lr 5.00 | ms/batch 155.85 | loss  7.54 | ppl  1890.85\n",
      "| epoch   1 |    54/ 1320 batches | lr 5.00 | ms/batch 149.76 | loss  7.94 | ppl  2808.37\n",
      "| epoch   1 |    55/ 1320 batches | lr 5.00 | ms/batch 156.83 | loss  6.87 | ppl   967.08\n",
      "| epoch   1 |    56/ 1320 batches | lr 5.00 | ms/batch 166.47 | loss  7.73 | ppl  2268.81\n",
      "| epoch   1 |    57/ 1320 batches | lr 5.00 | ms/batch 168.65 | loss  7.40 | ppl  1637.13\n",
      "| epoch   1 |    58/ 1320 batches | lr 5.00 | ms/batch 151.89 | loss  9.41 | ppl 12248.23\n",
      "| epoch   1 |    59/ 1320 batches | lr 5.00 | ms/batch 166.23 | loss  7.87 | ppl  2628.80\n",
      "| epoch   1 |    60/ 1320 batches | lr 5.00 | ms/batch 151.99 | loss  7.67 | ppl  2149.09\n",
      "| epoch   1 |    61/ 1320 batches | lr 5.00 | ms/batch 152.81 | loss  7.64 | ppl  2085.48\n",
      "| epoch   1 |    62/ 1320 batches | lr 5.00 | ms/batch 155.78 | loss  7.19 | ppl  1324.14\n",
      "| epoch   1 |    63/ 1320 batches | lr 5.00 | ms/batch 156.36 | loss  7.12 | ppl  1238.59\n",
      "| epoch   1 |    64/ 1320 batches | lr 5.00 | ms/batch 156.20 | loss  9.24 | ppl 10257.60\n",
      "| epoch   1 |    65/ 1320 batches | lr 5.00 | ms/batch 154.47 | loss  7.27 | ppl  1435.79\n",
      "| epoch   1 |    66/ 1320 batches | lr 5.00 | ms/batch 158.99 | loss  7.39 | ppl  1623.34\n",
      "| epoch   1 |    67/ 1320 batches | lr 5.00 | ms/batch 156.37 | loss  7.56 | ppl  1915.54\n",
      "| epoch   1 |    68/ 1320 batches | lr 5.00 | ms/batch 155.07 | loss  7.55 | ppl  1898.46\n",
      "| epoch   1 |    69/ 1320 batches | lr 5.00 | ms/batch 154.10 | loss  6.77 | ppl   870.81\n",
      "| epoch   1 |    70/ 1320 batches | lr 5.00 | ms/batch 155.85 | loss  7.43 | ppl  1680.65\n",
      "| epoch   1 |    71/ 1320 batches | lr 5.00 | ms/batch 154.15 | loss  7.30 | ppl  1480.80\n",
      "| epoch   1 |    72/ 1320 batches | lr 5.00 | ms/batch 158.09 | loss  7.11 | ppl  1226.24\n",
      "| epoch   1 |    73/ 1320 batches | lr 5.00 | ms/batch 157.17 | loss  7.31 | ppl  1493.47\n",
      "| epoch   1 |    74/ 1320 batches | lr 5.00 | ms/batch 218.23 | loss  7.39 | ppl  1623.38\n",
      "| epoch   1 |    75/ 1320 batches | lr 5.00 | ms/batch 154.92 | loss  6.97 | ppl  1066.55\n",
      "| epoch   1 |    76/ 1320 batches | lr 5.00 | ms/batch 155.29 | loss  6.97 | ppl  1065.97\n",
      "| epoch   1 |    77/ 1320 batches | lr 5.00 | ms/batch 155.77 | loss  7.41 | ppl  1659.77\n",
      "| epoch   1 |    78/ 1320 batches | lr 5.00 | ms/batch 165.95 | loss  8.35 | ppl  4244.29\n",
      "| epoch   1 |    79/ 1320 batches | lr 5.00 | ms/batch 155.14 | loss  6.97 | ppl  1060.74\n",
      "| epoch   1 |    80/ 1320 batches | lr 5.00 | ms/batch 156.22 | loss  7.91 | ppl  2713.25\n",
      "| epoch   1 |    81/ 1320 batches | lr 5.00 | ms/batch 153.57 | loss  7.21 | ppl  1354.19\n",
      "| epoch   1 |    82/ 1320 batches | lr 5.00 | ms/batch 159.43 | loss  7.02 | ppl  1115.61\n",
      "| epoch   1 |    83/ 1320 batches | lr 5.00 | ms/batch 154.39 | loss  7.06 | ppl  1162.15\n",
      "| epoch   1 |    84/ 1320 batches | lr 5.00 | ms/batch 155.20 | loss  8.57 | ppl  5282.59\n",
      "| epoch   1 |    85/ 1320 batches | lr 5.00 | ms/batch 153.13 | loss  7.28 | ppl  1457.36\n",
      "| epoch   1 |    86/ 1320 batches | lr 5.00 | ms/batch 153.46 | loss  7.34 | ppl  1536.89\n",
      "| epoch   1 |    87/ 1320 batches | lr 5.00 | ms/batch 155.94 | loss  7.96 | ppl  2850.92\n",
      "| epoch   1 |    88/ 1320 batches | lr 5.00 | ms/batch 152.96 | loss  7.08 | ppl  1188.08\n",
      "| epoch   1 |    89/ 1320 batches | lr 5.00 | ms/batch 152.07 | loss  7.30 | ppl  1480.21\n",
      "| epoch   1 |    90/ 1320 batches | lr 5.00 | ms/batch 155.84 | loss  7.27 | ppl  1437.01\n",
      "| epoch   1 |    91/ 1320 batches | lr 5.00 | ms/batch 152.54 | loss  6.81 | ppl   910.48\n",
      "| epoch   1 |    92/ 1320 batches | lr 5.00 | ms/batch 154.01 | loss  6.98 | ppl  1072.14\n",
      "| epoch   1 |    93/ 1320 batches | lr 5.00 | ms/batch 154.09 | loss  7.48 | ppl  1770.42\n",
      "| epoch   1 |    94/ 1320 batches | lr 5.00 | ms/batch 154.04 | loss  7.18 | ppl  1315.13\n",
      "| epoch   1 |    95/ 1320 batches | lr 5.00 | ms/batch 162.96 | loss  6.89 | ppl   980.53\n",
      "| epoch   1 |    96/ 1320 batches | lr 5.00 | ms/batch 155.03 | loss  7.64 | ppl  2087.39\n",
      "| epoch   1 |    97/ 1320 batches | lr 5.00 | ms/batch 152.24 | loss  7.11 | ppl  1224.68\n",
      "| epoch   1 |    98/ 1320 batches | lr 5.00 | ms/batch 152.17 | loss  8.23 | ppl  3751.12\n",
      "| epoch   1 |    99/ 1320 batches | lr 5.00 | ms/batch 153.76 | loss  7.14 | ppl  1255.96\n",
      "| epoch   1 |   100/ 1320 batches | lr 5.00 | ms/batch 153.99 | loss  7.31 | ppl  1492.88\n",
      "| epoch   1 |   101/ 1320 batches | lr 5.00 | ms/batch 154.31 | loss  7.78 | ppl  2381.92\n",
      "| epoch   1 |   102/ 1320 batches | lr 5.00 | ms/batch 153.02 | loss  7.26 | ppl  1424.27\n",
      "| epoch   1 |   103/ 1320 batches | lr 5.00 | ms/batch 151.52 | loss  6.61 | ppl   743.55\n",
      "| epoch   1 |   104/ 1320 batches | lr 5.00 | ms/batch 154.56 | loss  7.02 | ppl  1115.22\n",
      "| epoch   1 |   105/ 1320 batches | lr 5.00 | ms/batch 154.81 | loss  7.21 | ppl  1346.72\n",
      "| epoch   1 |   106/ 1320 batches | lr 5.00 | ms/batch 152.34 | loss  7.42 | ppl  1672.45\n",
      "| epoch   1 |   107/ 1320 batches | lr 5.00 | ms/batch 153.60 | loss  7.34 | ppl  1537.70\n",
      "| epoch   1 |   108/ 1320 batches | lr 5.00 | ms/batch 152.65 | loss  6.76 | ppl   865.03\n",
      "| epoch   1 |   109/ 1320 batches | lr 5.00 | ms/batch 203.24 | loss  7.16 | ppl  1287.41\n",
      "| epoch   1 |   110/ 1320 batches | lr 5.00 | ms/batch 153.63 | loss  7.19 | ppl  1319.64\n",
      "| epoch   1 |   111/ 1320 batches | lr 5.00 | ms/batch 153.33 | loss  8.30 | ppl  4036.23\n",
      "| epoch   1 |   112/ 1320 batches | lr 5.00 | ms/batch 155.25 | loss  7.39 | ppl  1626.72\n",
      "| epoch   1 |   113/ 1320 batches | lr 5.00 | ms/batch 155.52 | loss  7.42 | ppl  1672.91\n",
      "| epoch   1 |   114/ 1320 batches | lr 5.00 | ms/batch 152.88 | loss  6.95 | ppl  1039.27\n",
      "| epoch   1 |   115/ 1320 batches | lr 5.00 | ms/batch 152.64 | loss  6.84 | ppl   935.98\n",
      "| epoch   1 |   116/ 1320 batches | lr 5.00 | ms/batch 152.59 | loss  7.10 | ppl  1215.66\n",
      "| epoch   1 |   117/ 1320 batches | lr 5.00 | ms/batch 154.55 | loss  6.84 | ppl   936.17\n",
      "| epoch   1 |   118/ 1320 batches | lr 5.00 | ms/batch 153.18 | loss  7.90 | ppl  2709.35\n",
      "| epoch   1 |   119/ 1320 batches | lr 5.00 | ms/batch 154.19 | loss  7.09 | ppl  1203.11\n",
      "| epoch   1 |   120/ 1320 batches | lr 5.00 | ms/batch 149.12 | loss  6.90 | ppl   993.24\n",
      "| epoch   1 |   121/ 1320 batches | lr 5.00 | ms/batch 156.93 | loss  7.52 | ppl  1849.53\n",
      "| epoch   1 |   122/ 1320 batches | lr 5.00 | ms/batch 150.74 | loss  7.17 | ppl  1303.63\n",
      "| epoch   1 |   123/ 1320 batches | lr 5.00 | ms/batch 170.87 | loss  6.84 | ppl   937.91\n",
      "| epoch   1 |   124/ 1320 batches | lr 5.00 | ms/batch 149.14 | loss  7.50 | ppl  1799.17\n",
      "| epoch   1 |   125/ 1320 batches | lr 5.00 | ms/batch 153.83 | loss  6.72 | ppl   830.06\n",
      "| epoch   1 |   126/ 1320 batches | lr 5.00 | ms/batch 150.28 | loss  6.55 | ppl   698.07\n",
      "| epoch   1 |   127/ 1320 batches | lr 5.00 | ms/batch 148.75 | loss  6.77 | ppl   867.11\n",
      "| epoch   1 |   128/ 1320 batches | lr 5.00 | ms/batch 148.26 | loss  7.16 | ppl  1290.03\n",
      "| epoch   1 |   129/ 1320 batches | lr 5.00 | ms/batch 151.23 | loss  6.56 | ppl   705.49\n",
      "| epoch   1 |   130/ 1320 batches | lr 5.00 | ms/batch 149.24 | loss  7.05 | ppl  1152.48\n",
      "| epoch   1 |   131/ 1320 batches | lr 5.00 | ms/batch 177.26 | loss  6.85 | ppl   940.54\n",
      "| epoch   1 |   132/ 1320 batches | lr 5.00 | ms/batch 156.01 | loss  6.79 | ppl   885.26\n",
      "| epoch   1 |   133/ 1320 batches | lr 5.00 | ms/batch 152.79 | loss  6.70 | ppl   810.31\n",
      "| epoch   1 |   134/ 1320 batches | lr 5.00 | ms/batch 152.71 | loss  7.27 | ppl  1429.88\n",
      "| epoch   1 |   135/ 1320 batches | lr 5.00 | ms/batch 155.68 | loss  6.66 | ppl   778.62\n",
      "| epoch   1 |   136/ 1320 batches | lr 5.00 | ms/batch 161.35 | loss  6.60 | ppl   737.58\n",
      "| epoch   1 |   137/ 1320 batches | lr 5.00 | ms/batch 164.17 | loss  6.59 | ppl   726.65\n",
      "| epoch   1 |   138/ 1320 batches | lr 5.00 | ms/batch 153.97 | loss  6.72 | ppl   827.69\n",
      "| epoch   1 |   139/ 1320 batches | lr 5.00 | ms/batch 154.96 | loss  6.75 | ppl   852.26\n",
      "| epoch   1 |   140/ 1320 batches | lr 5.00 | ms/batch 165.71 | loss  7.12 | ppl  1242.51\n",
      "| epoch   1 |   141/ 1320 batches | lr 5.00 | ms/batch 156.89 | loss  7.07 | ppl  1180.43\n",
      "| epoch   1 |   142/ 1320 batches | lr 5.00 | ms/batch 152.47 | loss  7.03 | ppl  1128.54\n",
      "| epoch   1 |   143/ 1320 batches | lr 5.00 | ms/batch 153.88 | loss  6.79 | ppl   891.05\n",
      "| epoch   1 |   144/ 1320 batches | lr 5.00 | ms/batch 153.41 | loss  7.85 | ppl  2557.35\n",
      "| epoch   1 |   145/ 1320 batches | lr 5.00 | ms/batch 152.25 | loss  7.14 | ppl  1264.69\n",
      "| epoch   1 |   146/ 1320 batches | lr 5.00 | ms/batch 152.93 | loss  7.16 | ppl  1285.42\n",
      "| epoch   1 |   147/ 1320 batches | lr 5.00 | ms/batch 152.22 | loss  6.53 | ppl   685.30\n",
      "| epoch   1 |   148/ 1320 batches | lr 5.00 | ms/batch 155.28 | loss  6.57 | ppl   710.22\n",
      "| epoch   1 |   149/ 1320 batches | lr 5.00 | ms/batch 152.21 | loss  6.72 | ppl   828.88\n",
      "| epoch   1 |   150/ 1320 batches | lr 5.00 | ms/batch 152.18 | loss  6.69 | ppl   801.95\n",
      "| epoch   1 |   151/ 1320 batches | lr 5.00 | ms/batch 158.85 | loss  6.44 | ppl   628.19\n",
      "| epoch   1 |   152/ 1320 batches | lr 5.00 | ms/batch 154.69 | loss  6.77 | ppl   867.78\n",
      "| epoch   1 |   153/ 1320 batches | lr 5.00 | ms/batch 150.81 | loss  6.54 | ppl   692.55\n",
      "| epoch   1 |   154/ 1320 batches | lr 5.00 | ms/batch 152.41 | loss  6.94 | ppl  1030.12\n",
      "| epoch   1 |   155/ 1320 batches | lr 5.00 | ms/batch 153.11 | loss  7.05 | ppl  1153.33\n",
      "| epoch   1 |   156/ 1320 batches | lr 5.00 | ms/batch 153.00 | loss  6.78 | ppl   876.19\n",
      "| epoch   1 |   157/ 1320 batches | lr 5.00 | ms/batch 159.10 | loss  6.79 | ppl   889.09\n",
      "| epoch   1 |   158/ 1320 batches | lr 5.00 | ms/batch 156.82 | loss  6.77 | ppl   868.91\n",
      "| epoch   1 |   159/ 1320 batches | lr 5.00 | ms/batch 153.87 | loss  6.82 | ppl   920.10\n",
      "| epoch   1 |   160/ 1320 batches | lr 5.00 | ms/batch 151.31 | loss  7.03 | ppl  1127.62\n",
      "| epoch   1 |   161/ 1320 batches | lr 5.00 | ms/batch 152.94 | loss  6.88 | ppl   971.79\n",
      "| epoch   1 |   162/ 1320 batches | lr 5.00 | ms/batch 150.10 | loss  6.49 | ppl   656.78\n",
      "| epoch   1 |   163/ 1320 batches | lr 5.00 | ms/batch 151.78 | loss  6.78 | ppl   876.84\n",
      "| epoch   1 |   164/ 1320 batches | lr 5.00 | ms/batch 150.66 | loss  6.75 | ppl   855.76\n",
      "| epoch   1 |   165/ 1320 batches | lr 5.00 | ms/batch 155.17 | loss  6.73 | ppl   837.59\n",
      "| epoch   1 |   166/ 1320 batches | lr 5.00 | ms/batch 149.69 | loss  6.38 | ppl   589.46\n",
      "| epoch   1 |   167/ 1320 batches | lr 5.00 | ms/batch 169.89 | loss  6.58 | ppl   721.48\n",
      "| epoch   1 |   168/ 1320 batches | lr 5.00 | ms/batch 179.35 | loss  6.63 | ppl   753.85\n",
      "| epoch   1 |   169/ 1320 batches | lr 5.00 | ms/batch 172.82 | loss  6.89 | ppl   986.25\n",
      "| epoch   1 |   170/ 1320 batches | lr 5.00 | ms/batch 171.67 | loss  6.74 | ppl   848.68\n",
      "| epoch   1 |   171/ 1320 batches | lr 5.00 | ms/batch 217.41 | loss  6.76 | ppl   865.83\n",
      "| epoch   1 |   172/ 1320 batches | lr 5.00 | ms/batch 153.68 | loss  7.17 | ppl  1305.82\n",
      "| epoch   1 |   173/ 1320 batches | lr 5.00 | ms/batch 161.14 | loss  6.58 | ppl   721.25\n",
      "| epoch   1 |   174/ 1320 batches | lr 5.00 | ms/batch 195.51 | loss  6.73 | ppl   839.25\n",
      "| epoch   1 |   175/ 1320 batches | lr 5.00 | ms/batch 171.05 | loss  6.64 | ppl   768.13\n",
      "| epoch   1 |   176/ 1320 batches | lr 5.00 | ms/batch 188.57 | loss  6.91 | ppl  1002.10\n",
      "| epoch   1 |   177/ 1320 batches | lr 5.00 | ms/batch 163.57 | loss  6.66 | ppl   777.95\n",
      "| epoch   1 |   178/ 1320 batches | lr 5.00 | ms/batch 177.98 | loss  6.95 | ppl  1043.72\n",
      "| epoch   1 |   179/ 1320 batches | lr 5.00 | ms/batch 213.20 | loss  6.61 | ppl   741.71\n",
      "| epoch   1 |   180/ 1320 batches | lr 5.00 | ms/batch 174.94 | loss  6.70 | ppl   809.22\n",
      "| epoch   1 |   181/ 1320 batches | lr 5.00 | ms/batch 218.57 | loss  6.85 | ppl   947.71\n",
      "| epoch   1 |   182/ 1320 batches | lr 5.00 | ms/batch 171.79 | loss  6.53 | ppl   688.00\n",
      "| epoch   1 |   183/ 1320 batches | lr 5.00 | ms/batch 158.01 | loss  6.65 | ppl   774.87\n",
      "| epoch   1 |   184/ 1320 batches | lr 5.00 | ms/batch 164.78 | loss  6.92 | ppl  1008.17\n",
      "| epoch   1 |   185/ 1320 batches | lr 5.00 | ms/batch 199.95 | loss  6.79 | ppl   885.20\n",
      "| epoch   1 |   186/ 1320 batches | lr 5.00 | ms/batch 183.72 | loss  6.71 | ppl   820.49\n",
      "| epoch   1 |   187/ 1320 batches | lr 5.00 | ms/batch 216.68 | loss  6.87 | ppl   963.98\n",
      "| epoch   1 |   188/ 1320 batches | lr 5.00 | ms/batch 168.80 | loss  7.54 | ppl  1878.83\n",
      "| epoch   1 |   189/ 1320 batches | lr 5.00 | ms/batch 153.15 | loss  6.81 | ppl   909.88\n",
      "| epoch   1 |   190/ 1320 batches | lr 5.00 | ms/batch 185.90 | loss  6.79 | ppl   892.54\n",
      "| epoch   1 |   191/ 1320 batches | lr 5.00 | ms/batch 271.86 | loss  6.78 | ppl   878.09\n",
      "| epoch   1 |   192/ 1320 batches | lr 5.00 | ms/batch 195.11 | loss  6.57 | ppl   710.69\n",
      "| epoch   1 |   193/ 1320 batches | lr 5.00 | ms/batch 163.70 | loss  6.99 | ppl  1087.32\n",
      "| epoch   1 |   194/ 1320 batches | lr 5.00 | ms/batch 178.44 | loss  6.60 | ppl   731.53\n",
      "| epoch   1 |   195/ 1320 batches | lr 5.00 | ms/batch 175.49 | loss  6.30 | ppl   546.52\n",
      "| epoch   1 |   196/ 1320 batches | lr 5.00 | ms/batch 171.13 | loss  6.51 | ppl   671.83\n",
      "| epoch   1 |   197/ 1320 batches | lr 5.00 | ms/batch 171.43 | loss  6.48 | ppl   654.99\n",
      "| epoch   1 |   198/ 1320 batches | lr 5.00 | ms/batch 153.33 | loss  6.99 | ppl  1084.71\n",
      "| epoch   1 |   199/ 1320 batches | lr 5.00 | ms/batch 155.26 | loss  6.84 | ppl   938.36\n",
      "| epoch   1 |   200/ 1320 batches | lr 5.00 | ms/batch 152.65 | loss  6.62 | ppl   751.79\n",
      "| epoch   1 |   201/ 1320 batches | lr 5.00 | ms/batch 151.87 | loss  6.66 | ppl   782.16\n",
      "| epoch   1 |   202/ 1320 batches | lr 5.00 | ms/batch 149.11 | loss  6.62 | ppl   749.24\n",
      "| epoch   1 |   203/ 1320 batches | lr 5.00 | ms/batch 149.33 | loss  6.51 | ppl   672.43\n",
      "| epoch   1 |   204/ 1320 batches | lr 5.00 | ms/batch 146.84 | loss  6.66 | ppl   784.03\n",
      "| epoch   1 |   205/ 1320 batches | lr 5.00 | ms/batch 157.71 | loss  6.89 | ppl   983.27\n",
      "| epoch   1 |   206/ 1320 batches | lr 5.00 | ms/batch 154.00 | loss  6.59 | ppl   728.77\n",
      "| epoch   1 |   207/ 1320 batches | lr 5.00 | ms/batch 160.90 | loss  6.48 | ppl   650.75\n",
      "| epoch   1 |   208/ 1320 batches | lr 5.00 | ms/batch 179.09 | loss  6.49 | ppl   661.82\n",
      "| epoch   1 |   209/ 1320 batches | lr 5.00 | ms/batch 173.58 | loss  6.82 | ppl   920.28\n",
      "| epoch   1 |   210/ 1320 batches | lr 5.00 | ms/batch 183.66 | loss  6.20 | ppl   493.27\n",
      "| epoch   1 |   211/ 1320 batches | lr 5.00 | ms/batch 161.59 | loss  6.67 | ppl   790.00\n",
      "| epoch   1 |   212/ 1320 batches | lr 5.00 | ms/batch 259.12 | loss  6.60 | ppl   738.58\n",
      "| epoch   1 |   213/ 1320 batches | lr 5.00 | ms/batch 198.42 | loss  6.76 | ppl   858.80\n",
      "| epoch   1 |   214/ 1320 batches | lr 5.00 | ms/batch 160.67 | loss  6.52 | ppl   678.64\n",
      "| epoch   1 |   215/ 1320 batches | lr 5.00 | ms/batch 204.50 | loss  6.44 | ppl   629.40\n",
      "| epoch   1 |   216/ 1320 batches | lr 5.00 | ms/batch 176.96 | loss  6.20 | ppl   491.71\n",
      "| epoch   1 |   217/ 1320 batches | lr 5.00 | ms/batch 160.30 | loss  6.33 | ppl   560.72\n",
      "| epoch   1 |   218/ 1320 batches | lr 5.00 | ms/batch 164.72 | loss  6.41 | ppl   606.97\n",
      "| epoch   1 |   219/ 1320 batches | lr 5.00 | ms/batch 196.74 | loss  6.86 | ppl   955.72\n",
      "| epoch   1 |   220/ 1320 batches | lr 5.00 | ms/batch 182.42 | loss  6.70 | ppl   814.94\n",
      "| epoch   1 |   221/ 1320 batches | lr 5.00 | ms/batch 171.63 | loss  6.56 | ppl   702.90\n",
      "| epoch   1 |   222/ 1320 batches | lr 5.00 | ms/batch 155.95 | loss  6.65 | ppl   776.62\n",
      "| epoch   1 |   223/ 1320 batches | lr 5.00 | ms/batch 153.67 | loss  6.93 | ppl  1018.25\n",
      "| epoch   1 |   224/ 1320 batches | lr 5.00 | ms/batch 151.38 | loss  6.48 | ppl   651.57\n",
      "| epoch   1 |   225/ 1320 batches | lr 5.00 | ms/batch 152.68 | loss  6.73 | ppl   833.31\n",
      "| epoch   1 |   226/ 1320 batches | lr 5.00 | ms/batch 156.29 | loss  6.48 | ppl   651.65\n",
      "| epoch   1 |   227/ 1320 batches | lr 5.00 | ms/batch 156.81 | loss  6.53 | ppl   683.20\n",
      "| epoch   1 |   228/ 1320 batches | lr 5.00 | ms/batch 168.05 | loss  6.44 | ppl   624.38\n",
      "| epoch   1 |   229/ 1320 batches | lr 5.00 | ms/batch 178.95 | loss  6.26 | ppl   524.56\n",
      "| epoch   1 |   230/ 1320 batches | lr 5.00 | ms/batch 161.62 | loss  6.36 | ppl   577.16\n",
      "| epoch   1 |   231/ 1320 batches | lr 5.00 | ms/batch 157.75 | loss  6.51 | ppl   671.51\n",
      "| epoch   1 |   232/ 1320 batches | lr 5.00 | ms/batch 162.94 | loss  6.60 | ppl   736.47\n",
      "| epoch   1 |   233/ 1320 batches | lr 5.00 | ms/batch 165.87 | loss  6.90 | ppl   990.79\n",
      "| epoch   1 |   234/ 1320 batches | lr 5.00 | ms/batch 160.87 | loss  6.65 | ppl   773.71\n",
      "| epoch   1 |   235/ 1320 batches | lr 5.00 | ms/batch 160.95 | loss  6.32 | ppl   556.18\n",
      "| epoch   1 |   236/ 1320 batches | lr 5.00 | ms/batch 164.46 | loss  6.78 | ppl   880.38\n",
      "| epoch   1 |   237/ 1320 batches | lr 5.00 | ms/batch 170.55 | loss  6.73 | ppl   836.92\n",
      "| epoch   1 |   238/ 1320 batches | lr 5.00 | ms/batch 164.81 | loss  6.58 | ppl   718.75\n",
      "| epoch   1 |   239/ 1320 batches | lr 5.00 | ms/batch 164.40 | loss  6.64 | ppl   768.04\n",
      "| epoch   1 |   240/ 1320 batches | lr 5.00 | ms/batch 162.42 | loss  6.67 | ppl   792.27\n",
      "| epoch   1 |   241/ 1320 batches | lr 5.00 | ms/batch 169.49 | loss  6.76 | ppl   859.39\n",
      "| epoch   1 |   242/ 1320 batches | lr 5.00 | ms/batch 169.73 | loss  6.50 | ppl   666.28\n",
      "| epoch   1 |   243/ 1320 batches | lr 5.00 | ms/batch 168.54 | loss  6.61 | ppl   739.30\n",
      "| epoch   1 |   244/ 1320 batches | lr 5.00 | ms/batch 175.33 | loss  6.79 | ppl   885.93\n",
      "| epoch   1 |   245/ 1320 batches | lr 5.00 | ms/batch 173.68 | loss  6.84 | ppl   938.26\n",
      "| epoch   1 |   246/ 1320 batches | lr 5.00 | ms/batch 159.56 | loss  6.54 | ppl   694.79\n",
      "| epoch   1 |   247/ 1320 batches | lr 5.00 | ms/batch 187.33 | loss  6.46 | ppl   637.13\n",
      "| epoch   1 |   248/ 1320 batches | lr 5.00 | ms/batch 184.21 | loss  6.46 | ppl   637.10\n",
      "| epoch   1 |   249/ 1320 batches | lr 5.00 | ms/batch 169.98 | loss  6.38 | ppl   589.81\n",
      "| epoch   1 |   250/ 1320 batches | lr 5.00 | ms/batch 153.86 | loss  6.48 | ppl   651.38\n",
      "| epoch   1 |   251/ 1320 batches | lr 5.00 | ms/batch 280.05 | loss  6.56 | ppl   705.05\n",
      "| epoch   1 |   252/ 1320 batches | lr 5.00 | ms/batch 181.38 | loss  6.47 | ppl   646.89\n",
      "| epoch   1 |   253/ 1320 batches | lr 5.00 | ms/batch 235.34 | loss  6.56 | ppl   708.50\n",
      "| epoch   1 |   254/ 1320 batches | lr 5.00 | ms/batch 188.86 | loss  6.61 | ppl   745.85\n",
      "| epoch   1 |   255/ 1320 batches | lr 5.00 | ms/batch 203.29 | loss  6.52 | ppl   676.91\n",
      "| epoch   1 |   256/ 1320 batches | lr 5.00 | ms/batch 192.39 | loss  6.70 | ppl   810.10\n",
      "| epoch   1 |   257/ 1320 batches | lr 5.00 | ms/batch 196.35 | loss  6.47 | ppl   645.03\n",
      "| epoch   1 |   258/ 1320 batches | lr 5.00 | ms/batch 165.70 | loss  6.52 | ppl   681.29\n",
      "| epoch   1 |   259/ 1320 batches | lr 5.00 | ms/batch 171.39 | loss  6.60 | ppl   735.04\n",
      "| epoch   1 |   260/ 1320 batches | lr 5.00 | ms/batch 174.19 | loss  6.60 | ppl   737.60\n",
      "| epoch   1 |   261/ 1320 batches | lr 5.00 | ms/batch 168.67 | loss  6.52 | ppl   677.98\n",
      "| epoch   1 |   262/ 1320 batches | lr 5.00 | ms/batch 169.45 | loss  6.60 | ppl   733.85\n",
      "| epoch   1 |   263/ 1320 batches | lr 5.00 | ms/batch 169.76 | loss  6.55 | ppl   697.10\n",
      "| epoch   1 |   264/ 1320 batches | lr 5.00 | ms/batch 168.38 | loss  6.44 | ppl   627.55\n",
      "| epoch   1 |   265/ 1320 batches | lr 5.00 | ms/batch 178.36 | loss  6.61 | ppl   744.13\n",
      "| epoch   1 |   266/ 1320 batches | lr 5.00 | ms/batch 163.74 | loss  6.60 | ppl   734.99\n",
      "| epoch   1 |   267/ 1320 batches | lr 5.00 | ms/batch 160.20 | loss  6.46 | ppl   635.89\n",
      "| epoch   1 |   268/ 1320 batches | lr 5.00 | ms/batch 161.94 | loss  6.53 | ppl   687.38\n",
      "| epoch   1 |   269/ 1320 batches | lr 5.00 | ms/batch 160.10 | loss  6.58 | ppl   717.12\n",
      "| epoch   1 |   270/ 1320 batches | lr 5.00 | ms/batch 180.37 | loss  6.51 | ppl   672.12\n",
      "| epoch   1 |   271/ 1320 batches | lr 5.00 | ms/batch 159.69 | loss  6.42 | ppl   613.85\n",
      "| epoch   1 |   272/ 1320 batches | lr 5.00 | ms/batch 163.07 | loss  6.36 | ppl   579.88\n",
      "| epoch   1 |   273/ 1320 batches | lr 5.00 | ms/batch 155.60 | loss  6.44 | ppl   627.98\n",
      "| epoch   1 |   274/ 1320 batches | lr 5.00 | ms/batch 148.47 | loss  6.28 | ppl   533.94\n",
      "| epoch   1 |   275/ 1320 batches | lr 5.00 | ms/batch 152.17 | loss  6.33 | ppl   563.15\n",
      "| epoch   1 |   276/ 1320 batches | lr 5.00 | ms/batch 159.29 | loss  6.59 | ppl   724.97\n",
      "| epoch   1 |   277/ 1320 batches | lr 5.00 | ms/batch 152.61 | loss  6.57 | ppl   714.30\n",
      "| epoch   1 |   278/ 1320 batches | lr 5.00 | ms/batch 148.44 | loss  6.36 | ppl   575.77\n",
      "| epoch   1 |   279/ 1320 batches | lr 5.00 | ms/batch 156.96 | loss  6.14 | ppl   465.79\n",
      "| epoch   1 |   280/ 1320 batches | lr 5.00 | ms/batch 158.17 | loss  6.04 | ppl   421.40\n",
      "| epoch   1 |   281/ 1320 batches | lr 5.00 | ms/batch 171.93 | loss  6.61 | ppl   743.59\n",
      "| epoch   1 |   282/ 1320 batches | lr 5.00 | ms/batch 161.26 | loss  6.46 | ppl   640.72\n",
      "| epoch   1 |   283/ 1320 batches | lr 5.00 | ms/batch 164.48 | loss  6.58 | ppl   723.06\n",
      "| epoch   1 |   284/ 1320 batches | lr 5.00 | ms/batch 165.28 | loss  6.71 | ppl   816.77\n",
      "| epoch   1 |   285/ 1320 batches | lr 5.00 | ms/batch 152.71 | loss  6.28 | ppl   532.33\n",
      "| epoch   1 |   286/ 1320 batches | lr 5.00 | ms/batch 169.87 | loss  6.37 | ppl   582.50\n",
      "| epoch   1 |   287/ 1320 batches | lr 5.00 | ms/batch 156.12 | loss  6.67 | ppl   789.83\n",
      "| epoch   1 |   288/ 1320 batches | lr 5.00 | ms/batch 167.34 | loss  6.38 | ppl   592.03\n",
      "| epoch   1 |   289/ 1320 batches | lr 5.00 | ms/batch 165.07 | loss  6.39 | ppl   597.15\n",
      "| epoch   1 |   290/ 1320 batches | lr 5.00 | ms/batch 164.35 | loss  6.57 | ppl   713.81\n",
      "| epoch   1 |   291/ 1320 batches | lr 5.00 | ms/batch 156.24 | loss  6.80 | ppl   901.44\n",
      "| epoch   1 |   292/ 1320 batches | lr 5.00 | ms/batch 156.46 | loss  6.34 | ppl   564.38\n",
      "| epoch   1 |   293/ 1320 batches | lr 5.00 | ms/batch 157.85 | loss  6.27 | ppl   529.54\n",
      "| epoch   1 |   294/ 1320 batches | lr 5.00 | ms/batch 163.24 | loss  6.13 | ppl   458.13\n",
      "| epoch   1 |   295/ 1320 batches | lr 5.00 | ms/batch 160.81 | loss  6.25 | ppl   520.32\n",
      "| epoch   1 |   296/ 1320 batches | lr 5.00 | ms/batch 167.60 | loss  6.41 | ppl   610.22\n",
      "| epoch   1 |   297/ 1320 batches | lr 5.00 | ms/batch 153.60 | loss  6.43 | ppl   621.20\n",
      "| epoch   1 |   298/ 1320 batches | lr 5.00 | ms/batch 165.07 | loss  6.61 | ppl   741.53\n",
      "| epoch   1 |   299/ 1320 batches | lr 5.00 | ms/batch 202.72 | loss  6.15 | ppl   466.56\n",
      "| epoch   1 |   300/ 1320 batches | lr 5.00 | ms/batch 158.80 | loss  6.31 | ppl   550.52\n",
      "| epoch   1 |   301/ 1320 batches | lr 5.00 | ms/batch 162.24 | loss  6.25 | ppl   518.12\n",
      "| epoch   1 |   302/ 1320 batches | lr 5.00 | ms/batch 164.48 | loss  6.07 | ppl   434.04\n",
      "| epoch   1 |   303/ 1320 batches | lr 5.00 | ms/batch 189.84 | loss  6.46 | ppl   637.52\n",
      "| epoch   1 |   304/ 1320 batches | lr 5.00 | ms/batch 180.32 | loss  6.47 | ppl   648.16\n",
      "| epoch   1 |   305/ 1320 batches | lr 5.00 | ms/batch 154.33 | loss  6.67 | ppl   786.84\n",
      "| epoch   1 |   306/ 1320 batches | lr 5.00 | ms/batch 159.81 | loss  6.42 | ppl   612.44\n",
      "| epoch   1 |   307/ 1320 batches | lr 5.00 | ms/batch 180.05 | loss  6.10 | ppl   445.60\n",
      "| epoch   1 |   308/ 1320 batches | lr 5.00 | ms/batch 181.88 | loss  5.91 | ppl   370.50\n",
      "| epoch   1 |   309/ 1320 batches | lr 5.00 | ms/batch 163.87 | loss  6.10 | ppl   447.84\n",
      "| epoch   1 |   310/ 1320 batches | lr 5.00 | ms/batch 163.06 | loss  6.51 | ppl   675.06\n",
      "| epoch   1 |   311/ 1320 batches | lr 5.00 | ms/batch 177.92 | loss  6.36 | ppl   580.08\n",
      "| epoch   1 |   312/ 1320 batches | lr 5.00 | ms/batch 151.82 | loss  6.20 | ppl   494.32\n",
      "| epoch   1 |   313/ 1320 batches | lr 5.00 | ms/batch 174.32 | loss  6.37 | ppl   584.56\n",
      "| epoch   1 |   314/ 1320 batches | lr 5.00 | ms/batch 164.19 | loss  6.15 | ppl   469.28\n",
      "| epoch   1 |   315/ 1320 batches | lr 5.00 | ms/batch 163.80 | loss  6.33 | ppl   559.66\n",
      "| epoch   1 |   316/ 1320 batches | lr 5.00 | ms/batch 156.20 | loss  6.19 | ppl   485.91\n",
      "| epoch   1 |   317/ 1320 batches | lr 5.00 | ms/batch 153.85 | loss  6.12 | ppl   455.10\n",
      "| epoch   1 |   318/ 1320 batches | lr 5.00 | ms/batch 156.51 | loss  6.14 | ppl   463.12\n",
      "| epoch   1 |   319/ 1320 batches | lr 5.00 | ms/batch 173.07 | loss  6.45 | ppl   633.91\n",
      "| epoch   1 |   320/ 1320 batches | lr 5.00 | ms/batch 164.64 | loss  6.69 | ppl   800.70\n",
      "| epoch   1 |   321/ 1320 batches | lr 5.00 | ms/batch 274.40 | loss  6.28 | ppl   534.12\n",
      "| epoch   1 |   322/ 1320 batches | lr 5.00 | ms/batch 172.11 | loss  6.15 | ppl   470.35\n",
      "| epoch   1 |   323/ 1320 batches | lr 5.00 | ms/batch 206.13 | loss  6.43 | ppl   623.26\n",
      "| epoch   1 |   324/ 1320 batches | lr 5.00 | ms/batch 175.62 | loss  6.36 | ppl   577.14\n",
      "| epoch   1 |   325/ 1320 batches | lr 5.00 | ms/batch 159.44 | loss  6.72 | ppl   829.84\n",
      "| epoch   1 |   326/ 1320 batches | lr 5.00 | ms/batch 263.03 | loss  6.24 | ppl   510.71\n",
      "| epoch   1 |   327/ 1320 batches | lr 5.00 | ms/batch 302.41 | loss  6.01 | ppl   407.69\n",
      "| epoch   1 |   328/ 1320 batches | lr 5.00 | ms/batch 291.84 | loss  6.43 | ppl   618.74\n",
      "| epoch   1 |   329/ 1320 batches | lr 5.00 | ms/batch 271.50 | loss  6.53 | ppl   687.16\n",
      "| epoch   1 |   330/ 1320 batches | lr 5.00 | ms/batch 331.22 | loss  6.02 | ppl   412.12\n",
      "| epoch   1 |   331/ 1320 batches | lr 5.00 | ms/batch 262.93 | loss  6.28 | ppl   536.28\n",
      "| epoch   1 |   332/ 1320 batches | lr 5.00 | ms/batch 400.14 | loss  6.49 | ppl   660.89\n",
      "| epoch   1 |   333/ 1320 batches | lr 5.00 | ms/batch 653.96 | loss  6.40 | ppl   603.51\n",
      "| epoch   1 |   334/ 1320 batches | lr 5.00 | ms/batch 384.64 | loss  6.40 | ppl   599.69\n",
      "| epoch   1 |   335/ 1320 batches | lr 5.00 | ms/batch 297.28 | loss  6.27 | ppl   530.09\n",
      "| epoch   1 |   336/ 1320 batches | lr 5.00 | ms/batch 267.45 | loss  6.33 | ppl   563.80\n",
      "| epoch   1 |   337/ 1320 batches | lr 5.00 | ms/batch 179.52 | loss  6.37 | ppl   582.92\n",
      "| epoch   1 |   338/ 1320 batches | lr 5.00 | ms/batch 180.26 | loss  6.61 | ppl   739.05\n",
      "| epoch   1 |   339/ 1320 batches | lr 5.00 | ms/batch 208.21 | loss  6.63 | ppl   758.47\n",
      "| epoch   1 |   340/ 1320 batches | lr 5.00 | ms/batch 177.14 | loss  6.21 | ppl   496.05\n",
      "| epoch   1 |   341/ 1320 batches | lr 5.00 | ms/batch 183.78 | loss  6.58 | ppl   720.66\n",
      "| epoch   1 |   342/ 1320 batches | lr 5.00 | ms/batch 325.82 | loss  6.59 | ppl   724.50\n",
      "| epoch   1 |   343/ 1320 batches | lr 5.00 | ms/batch 232.07 | loss  6.38 | ppl   588.84\n",
      "| epoch   1 |   344/ 1320 batches | lr 5.00 | ms/batch 302.11 | loss  6.64 | ppl   766.87\n",
      "| epoch   1 |   345/ 1320 batches | lr 5.00 | ms/batch 178.66 | loss  6.57 | ppl   711.23\n",
      "| epoch   1 |   346/ 1320 batches | lr 5.00 | ms/batch 197.95 | loss  6.41 | ppl   610.36\n",
      "| epoch   1 |   347/ 1320 batches | lr 5.00 | ms/batch 173.96 | loss  6.34 | ppl   564.89\n",
      "| epoch   1 |   348/ 1320 batches | lr 5.00 | ms/batch 185.12 | loss  6.39 | ppl   593.40\n",
      "| epoch   1 |   349/ 1320 batches | lr 5.00 | ms/batch 235.11 | loss  6.42 | ppl   616.87\n",
      "| epoch   1 |   350/ 1320 batches | lr 5.00 | ms/batch 165.42 | loss  6.36 | ppl   580.76\n",
      "| epoch   1 |   351/ 1320 batches | lr 5.00 | ms/batch 170.74 | loss  6.59 | ppl   728.77\n",
      "| epoch   1 |   352/ 1320 batches | lr 5.00 | ms/batch 175.52 | loss  6.32 | ppl   556.32\n",
      "| epoch   1 |   353/ 1320 batches | lr 5.00 | ms/batch 196.72 | loss  6.24 | ppl   513.53\n",
      "| epoch   1 |   354/ 1320 batches | lr 5.00 | ms/batch 194.49 | loss  6.71 | ppl   823.06\n",
      "| epoch   1 |   355/ 1320 batches | lr 5.00 | ms/batch 180.53 | loss  6.88 | ppl   970.39\n",
      "| epoch   1 |   356/ 1320 batches | lr 5.00 | ms/batch 232.80 | loss  6.22 | ppl   500.91\n",
      "| epoch   1 |   357/ 1320 batches | lr 5.00 | ms/batch 197.98 | loss  6.33 | ppl   562.40\n",
      "| epoch   1 |   358/ 1320 batches | lr 5.00 | ms/batch 184.34 | loss  6.41 | ppl   608.20\n",
      "| epoch   1 |   359/ 1320 batches | lr 5.00 | ms/batch 282.21 | loss  6.60 | ppl   733.59\n",
      "| epoch   1 |   360/ 1320 batches | lr 5.00 | ms/batch 499.83 | loss  6.45 | ppl   632.02\n",
      "| epoch   1 |   361/ 1320 batches | lr 5.00 | ms/batch 230.20 | loss  6.33 | ppl   563.08\n",
      "| epoch   1 |   362/ 1320 batches | lr 5.00 | ms/batch 201.20 | loss  6.63 | ppl   760.12\n",
      "| epoch   1 |   363/ 1320 batches | lr 5.00 | ms/batch 307.38 | loss  6.25 | ppl   518.48\n",
      "| epoch   1 |   364/ 1320 batches | lr 5.00 | ms/batch 249.09 | loss  6.07 | ppl   430.69\n",
      "| epoch   1 |   365/ 1320 batches | lr 5.00 | ms/batch 480.22 | loss  6.28 | ppl   531.99\n",
      "| epoch   1 |   366/ 1320 batches | lr 5.00 | ms/batch 269.48 | loss  6.13 | ppl   458.46\n",
      "| epoch   1 |   367/ 1320 batches | lr 5.00 | ms/batch 238.78 | loss  6.17 | ppl   476.04\n",
      "| epoch   1 |   368/ 1320 batches | lr 5.00 | ms/batch 219.46 | loss  6.31 | ppl   551.50\n",
      "| epoch   1 |   369/ 1320 batches | lr 5.00 | ms/batch 228.42 | loss  6.42 | ppl   615.32\n",
      "| epoch   1 |   370/ 1320 batches | lr 5.00 | ms/batch 249.04 | loss  6.32 | ppl   556.91\n",
      "| epoch   1 |   371/ 1320 batches | lr 5.00 | ms/batch 206.00 | loss  6.43 | ppl   621.21\n",
      "| epoch   1 |   372/ 1320 batches | lr 5.00 | ms/batch 195.69 | loss  6.20 | ppl   494.73\n",
      "| epoch   1 |   373/ 1320 batches | lr 5.00 | ms/batch 181.97 | loss  6.40 | ppl   601.35\n",
      "| epoch   1 |   374/ 1320 batches | lr 5.00 | ms/batch 248.35 | loss  6.41 | ppl   606.34\n",
      "| epoch   1 |   375/ 1320 batches | lr 5.00 | ms/batch 194.95 | loss  6.32 | ppl   557.41\n",
      "| epoch   1 |   376/ 1320 batches | lr 5.00 | ms/batch 184.47 | loss  6.11 | ppl   450.53\n",
      "| epoch   1 |   377/ 1320 batches | lr 5.00 | ms/batch 184.83 | loss  6.40 | ppl   602.51\n",
      "| epoch   1 |   378/ 1320 batches | lr 5.00 | ms/batch 182.66 | loss  6.34 | ppl   567.44\n",
      "| epoch   1 |   379/ 1320 batches | lr 5.00 | ms/batch 203.79 | loss  6.28 | ppl   532.14\n",
      "| epoch   1 |   380/ 1320 batches | lr 5.00 | ms/batch 246.82 | loss  6.06 | ppl   427.64\n",
      "| epoch   1 |   381/ 1320 batches | lr 5.00 | ms/batch 195.40 | loss  5.99 | ppl   401.25\n",
      "| epoch   1 |   382/ 1320 batches | lr 5.00 | ms/batch 188.14 | loss  6.40 | ppl   604.65\n",
      "| epoch   1 |   383/ 1320 batches | lr 5.00 | ms/batch 228.14 | loss  6.32 | ppl   555.31\n",
      "| epoch   1 |   384/ 1320 batches | lr 5.00 | ms/batch 186.15 | loss  6.24 | ppl   515.12\n",
      "| epoch   1 |   385/ 1320 batches | lr 5.00 | ms/batch 183.33 | loss  6.41 | ppl   610.71\n",
      "| epoch   1 |   386/ 1320 batches | lr 5.00 | ms/batch 183.47 | loss  6.02 | ppl   411.37\n",
      "| epoch   1 |   387/ 1320 batches | lr 5.00 | ms/batch 210.94 | loss  6.12 | ppl   456.74\n",
      "| epoch   1 |   388/ 1320 batches | lr 5.00 | ms/batch 267.86 | loss  6.44 | ppl   623.92\n",
      "| epoch   1 |   389/ 1320 batches | lr 5.00 | ms/batch 196.72 | loss  6.42 | ppl   614.31\n",
      "| epoch   1 |   390/ 1320 batches | lr 5.00 | ms/batch 243.33 | loss  6.18 | ppl   484.04\n",
      "| epoch   1 |   391/ 1320 batches | lr 5.00 | ms/batch 209.69 | loss  6.18 | ppl   483.86\n",
      "| epoch   1 |   392/ 1320 batches | lr 5.00 | ms/batch 225.57 | loss  6.23 | ppl   508.95\n",
      "| epoch   1 |   393/ 1320 batches | lr 5.00 | ms/batch 205.00 | loss  6.21 | ppl   497.20\n",
      "| epoch   1 |   394/ 1320 batches | lr 5.00 | ms/batch 250.99 | loss  6.55 | ppl   701.14\n",
      "| epoch   1 |   395/ 1320 batches | lr 5.00 | ms/batch 215.34 | loss  6.30 | ppl   545.26\n",
      "| epoch   1 |   396/ 1320 batches | lr 5.00 | ms/batch 234.32 | loss  6.36 | ppl   579.47\n",
      "| epoch   1 |   397/ 1320 batches | lr 5.00 | ms/batch 233.27 | loss  6.39 | ppl   597.42\n",
      "| epoch   1 |   398/ 1320 batches | lr 5.00 | ms/batch 227.79 | loss  6.00 | ppl   401.63\n",
      "| epoch   1 |   399/ 1320 batches | lr 5.00 | ms/batch 209.11 | loss  6.19 | ppl   489.01\n",
      "| epoch   1 |   400/ 1320 batches | lr 5.00 | ms/batch 255.80 | loss  6.10 | ppl   445.66\n",
      "| epoch   1 |   401/ 1320 batches | lr 5.00 | ms/batch 303.06 | loss  6.37 | ppl   582.62\n",
      "| epoch   1 |   402/ 1320 batches | lr 5.00 | ms/batch 236.41 | loss  6.30 | ppl   544.79\n",
      "| epoch   1 |   403/ 1320 batches | lr 5.00 | ms/batch 223.59 | loss  6.30 | ppl   546.15\n",
      "| epoch   1 |   404/ 1320 batches | lr 5.00 | ms/batch 319.14 | loss  6.26 | ppl   522.85\n",
      "| epoch   1 |   405/ 1320 batches | lr 5.00 | ms/batch 278.65 | loss  6.14 | ppl   465.40\n",
      "| epoch   1 |   406/ 1320 batches | lr 5.00 | ms/batch 228.90 | loss  6.06 | ppl   427.36\n",
      "| epoch   1 |   407/ 1320 batches | lr 5.00 | ms/batch 253.39 | loss  6.48 | ppl   648.99\n",
      "| epoch   1 |   408/ 1320 batches | lr 5.00 | ms/batch 244.94 | loss  6.37 | ppl   586.95\n",
      "| epoch   1 |   409/ 1320 batches | lr 5.00 | ms/batch 285.13 | loss  6.29 | ppl   540.82\n",
      "| epoch   1 |   410/ 1320 batches | lr 5.00 | ms/batch 261.06 | loss  6.18 | ppl   483.96\n",
      "| epoch   1 |   411/ 1320 batches | lr 5.00 | ms/batch 263.59 | loss  6.31 | ppl   547.42\n",
      "| epoch   1 |   412/ 1320 batches | lr 5.00 | ms/batch 270.55 | loss  6.19 | ppl   488.21\n",
      "| epoch   1 |   413/ 1320 batches | lr 5.00 | ms/batch 268.92 | loss  6.16 | ppl   471.36\n",
      "| epoch   1 |   414/ 1320 batches | lr 5.00 | ms/batch 270.47 | loss  6.03 | ppl   417.15\n",
      "| epoch   1 |   415/ 1320 batches | lr 5.00 | ms/batch 258.81 | loss  6.37 | ppl   581.19\n",
      "| epoch   1 |   416/ 1320 batches | lr 5.00 | ms/batch 265.31 | loss  6.00 | ppl   405.13\n",
      "| epoch   1 |   417/ 1320 batches | lr 5.00 | ms/batch 299.32 | loss  6.09 | ppl   440.96\n",
      "| epoch   1 |   418/ 1320 batches | lr 5.00 | ms/batch 277.50 | loss  6.51 | ppl   668.80\n",
      "| epoch   1 |   419/ 1320 batches | lr 5.00 | ms/batch 279.89 | loss  6.39 | ppl   596.00\n",
      "| epoch   1 |   420/ 1320 batches | lr 5.00 | ms/batch 273.32 | loss  6.00 | ppl   401.73\n",
      "| epoch   1 |   421/ 1320 batches | lr 5.00 | ms/batch 246.67 | loss  6.38 | ppl   590.39\n",
      "| epoch   1 |   422/ 1320 batches | lr 5.00 | ms/batch 257.43 | loss  6.34 | ppl   565.07\n",
      "| epoch   1 |   423/ 1320 batches | lr 5.00 | ms/batch 917.70 | loss  6.42 | ppl   612.64\n",
      "| epoch   1 |   424/ 1320 batches | lr 5.00 | ms/batch 481.50 | loss  6.55 | ppl   700.49\n",
      "| epoch   1 |   425/ 1320 batches | lr 5.00 | ms/batch 400.00 | loss  6.39 | ppl   595.07\n",
      "| epoch   1 |   426/ 1320 batches | lr 5.00 | ms/batch 604.48 | loss  6.28 | ppl   536.11\n",
      "| epoch   1 |   427/ 1320 batches | lr 5.00 | ms/batch 610.35 | loss  6.36 | ppl   579.45\n",
      "| epoch   1 |   428/ 1320 batches | lr 5.00 | ms/batch 457.57 | loss  6.11 | ppl   451.00\n",
      "| epoch   1 |   429/ 1320 batches | lr 5.00 | ms/batch 404.65 | loss  6.34 | ppl   568.60\n",
      "| epoch   1 |   430/ 1320 batches | lr 5.00 | ms/batch 322.87 | loss  6.36 | ppl   578.18\n",
      "| epoch   1 |   431/ 1320 batches | lr 5.00 | ms/batch 387.31 | loss  6.41 | ppl   606.40\n",
      "| epoch   1 |   432/ 1320 batches | lr 5.00 | ms/batch 434.51 | loss  6.44 | ppl   625.80\n",
      "| epoch   1 |   433/ 1320 batches | lr 5.00 | ms/batch 314.56 | loss  6.26 | ppl   520.79\n",
      "| epoch   1 |   434/ 1320 batches | lr 5.00 | ms/batch 306.75 | loss  6.29 | ppl   539.04\n",
      "| epoch   1 |   435/ 1320 batches | lr 5.00 | ms/batch 339.65 | loss  6.41 | ppl   605.65\n",
      "| epoch   1 |   436/ 1320 batches | lr 5.00 | ms/batch 412.62 | loss  6.10 | ppl   443.80\n",
      "| epoch   1 |   437/ 1320 batches | lr 5.00 | ms/batch 297.41 | loss  6.08 | ppl   435.15\n",
      "| epoch   1 |   438/ 1320 batches | lr 5.00 | ms/batch 331.43 | loss  6.10 | ppl   445.39\n",
      "| epoch   1 |   439/ 1320 batches | lr 5.00 | ms/batch 261.64 | loss  6.15 | ppl   469.97\n",
      "| epoch   1 |   440/ 1320 batches | lr 5.00 | ms/batch 327.70 | loss  6.14 | ppl   462.74\n",
      "| epoch   1 |   441/ 1320 batches | lr 5.00 | ms/batch 248.41 | loss  6.10 | ppl   447.23\n",
      "| epoch   1 |   442/ 1320 batches | lr 5.00 | ms/batch 238.77 | loss  6.12 | ppl   455.18\n",
      "| epoch   1 |   443/ 1320 batches | lr 5.00 | ms/batch 236.92 | loss  6.10 | ppl   446.74\n",
      "| epoch   1 |   444/ 1320 batches | lr 5.00 | ms/batch 219.30 | loss  6.04 | ppl   418.85\n",
      "| epoch   1 |   445/ 1320 batches | lr 5.00 | ms/batch 226.91 | loss  6.22 | ppl   500.87\n",
      "| epoch   1 |   446/ 1320 batches | lr 5.00 | ms/batch 212.04 | loss  6.31 | ppl   550.28\n",
      "| epoch   1 |   447/ 1320 batches | lr 5.00 | ms/batch 211.61 | loss  6.28 | ppl   531.82\n",
      "| epoch   1 |   448/ 1320 batches | lr 5.00 | ms/batch 193.76 | loss  5.47 | ppl   238.44\n",
      "| epoch   1 |   449/ 1320 batches | lr 5.00 | ms/batch 198.94 | loss  6.17 | ppl   479.19\n",
      "| epoch   1 |   450/ 1320 batches | lr 5.00 | ms/batch 203.97 | loss  6.65 | ppl   773.20\n",
      "| epoch   1 |   451/ 1320 batches | lr 5.00 | ms/batch 199.41 | loss  6.18 | ppl   484.64\n",
      "| epoch   1 |   452/ 1320 batches | lr 5.00 | ms/batch 189.38 | loss  6.23 | ppl   508.08\n",
      "| epoch   1 |   453/ 1320 batches | lr 5.00 | ms/batch 176.55 | loss  6.54 | ppl   689.46\n",
      "| epoch   1 |   454/ 1320 batches | lr 5.00 | ms/batch 177.25 | loss  6.09 | ppl   440.35\n",
      "| epoch   1 |   455/ 1320 batches | lr 5.00 | ms/batch 171.22 | loss  6.27 | ppl   529.76\n",
      "| epoch   1 |   456/ 1320 batches | lr 5.00 | ms/batch 178.39 | loss  6.27 | ppl   529.20\n",
      "| epoch   1 |   457/ 1320 batches | lr 5.00 | ms/batch 277.48 | loss  6.26 | ppl   524.69\n",
      "| epoch   1 |   458/ 1320 batches | lr 5.00 | ms/batch 326.20 | loss  6.27 | ppl   530.66\n",
      "| epoch   1 |   459/ 1320 batches | lr 5.00 | ms/batch 180.69 | loss  6.14 | ppl   463.81\n",
      "| epoch   1 |   460/ 1320 batches | lr 5.00 | ms/batch 169.09 | loss  6.09 | ppl   441.95\n",
      "| epoch   1 |   461/ 1320 batches | lr 5.00 | ms/batch 258.26 | loss  6.51 | ppl   669.40\n",
      "| epoch   1 |   462/ 1320 batches | lr 5.00 | ms/batch 285.15 | loss  6.34 | ppl   565.81\n",
      "| epoch   1 |   463/ 1320 batches | lr 5.00 | ms/batch 253.12 | loss  6.00 | ppl   401.93\n",
      "| epoch   1 |   464/ 1320 batches | lr 5.00 | ms/batch 177.24 | loss  5.90 | ppl   363.94\n",
      "| epoch   1 |   465/ 1320 batches | lr 5.00 | ms/batch 199.33 | loss  6.05 | ppl   425.56\n",
      "| epoch   1 |   466/ 1320 batches | lr 5.00 | ms/batch 203.53 | loss  6.15 | ppl   469.03\n",
      "| epoch   1 |   467/ 1320 batches | lr 5.00 | ms/batch 173.46 | loss  6.21 | ppl   500.13\n",
      "| epoch   1 |   468/ 1320 batches | lr 5.00 | ms/batch 207.87 | loss  6.16 | ppl   473.97\n",
      "| epoch   1 |   469/ 1320 batches | lr 5.00 | ms/batch 166.08 | loss  6.01 | ppl   408.66\n",
      "| epoch   1 |   470/ 1320 batches | lr 5.00 | ms/batch 206.01 | loss  6.07 | ppl   431.97\n",
      "| epoch   1 |   471/ 1320 batches | lr 5.00 | ms/batch 177.25 | loss  6.04 | ppl   418.21\n",
      "| epoch   1 |   472/ 1320 batches | lr 5.00 | ms/batch 179.35 | loss  6.23 | ppl   508.14\n",
      "| epoch   1 |   473/ 1320 batches | lr 5.00 | ms/batch 182.77 | loss  6.16 | ppl   474.29\n",
      "| epoch   1 |   474/ 1320 batches | lr 5.00 | ms/batch 187.30 | loss  6.10 | ppl   446.87\n",
      "| epoch   1 |   475/ 1320 batches | lr 5.00 | ms/batch 173.22 | loss  6.21 | ppl   499.19\n",
      "| epoch   1 |   476/ 1320 batches | lr 5.00 | ms/batch 269.13 | loss  6.34 | ppl   565.90\n",
      "| epoch   1 |   477/ 1320 batches | lr 5.00 | ms/batch 174.91 | loss  6.22 | ppl   503.82\n",
      "| epoch   1 |   478/ 1320 batches | lr 5.00 | ms/batch 172.94 | loss  6.47 | ppl   645.82\n",
      "| epoch   1 |   479/ 1320 batches | lr 5.00 | ms/batch 268.25 | loss  6.55 | ppl   699.45\n",
      "| epoch   1 |   480/ 1320 batches | lr 5.00 | ms/batch 166.08 | loss  6.36 | ppl   578.86\n",
      "| epoch   1 |   481/ 1320 batches | lr 5.00 | ms/batch 159.32 | loss  6.13 | ppl   457.26\n",
      "| epoch   1 |   482/ 1320 batches | lr 5.00 | ms/batch 162.12 | loss  6.24 | ppl   512.73\n",
      "| epoch   1 |   483/ 1320 batches | lr 5.00 | ms/batch 162.06 | loss  5.93 | ppl   376.71\n",
      "| epoch   1 |   484/ 1320 batches | lr 5.00 | ms/batch 158.40 | loss  6.08 | ppl   435.41\n",
      "| epoch   1 |   485/ 1320 batches | lr 5.00 | ms/batch 162.72 | loss  6.01 | ppl   407.75\n",
      "| epoch   1 |   486/ 1320 batches | lr 5.00 | ms/batch 171.78 | loss  6.00 | ppl   401.46\n",
      "| epoch   1 |   487/ 1320 batches | lr 5.00 | ms/batch 187.36 | loss  6.17 | ppl   479.98\n",
      "| epoch   1 |   488/ 1320 batches | lr 5.00 | ms/batch 180.17 | loss  5.77 | ppl   320.91\n",
      "| epoch   1 |   489/ 1320 batches | lr 5.00 | ms/batch 200.22 | loss  5.91 | ppl   367.05\n",
      "| epoch   1 |   490/ 1320 batches | lr 5.00 | ms/batch 194.46 | loss  5.94 | ppl   378.35\n",
      "| epoch   1 |   491/ 1320 batches | lr 5.00 | ms/batch 177.70 | loss  5.97 | ppl   390.98\n",
      "| epoch   1 |   492/ 1320 batches | lr 5.00 | ms/batch 192.51 | loss  6.09 | ppl   439.67\n",
      "| epoch   1 |   493/ 1320 batches | lr 5.00 | ms/batch 227.02 | loss  6.07 | ppl   434.18\n",
      "| epoch   1 |   494/ 1320 batches | lr 5.00 | ms/batch 271.93 | loss  6.30 | ppl   542.06\n",
      "| epoch   1 |   495/ 1320 batches | lr 5.00 | ms/batch 188.63 | loss  5.94 | ppl   381.63\n",
      "| epoch   1 |   496/ 1320 batches | lr 5.00 | ms/batch 167.58 | loss  6.03 | ppl   415.42\n",
      "| epoch   1 |   497/ 1320 batches | lr 5.00 | ms/batch 164.97 | loss  6.02 | ppl   411.47\n",
      "| epoch   1 |   498/ 1320 batches | lr 5.00 | ms/batch 288.40 | loss  6.10 | ppl   444.90\n",
      "| epoch   1 |   499/ 1320 batches | lr 5.00 | ms/batch 193.99 | loss  6.15 | ppl   467.80\n",
      "| epoch   1 |   500/ 1320 batches | lr 5.00 | ms/batch 168.10 | loss  6.40 | ppl   600.87\n",
      "| epoch   1 |   501/ 1320 batches | lr 5.00 | ms/batch 165.64 | loss  6.07 | ppl   431.50\n",
      "| epoch   1 |   502/ 1320 batches | lr 5.00 | ms/batch 162.14 | loss  6.28 | ppl   533.58\n",
      "| epoch   1 |   503/ 1320 batches | lr 5.00 | ms/batch 170.52 | loss  6.26 | ppl   525.56\n",
      "| epoch   1 |   504/ 1320 batches | lr 5.00 | ms/batch 187.30 | loss  6.36 | ppl   576.49\n",
      "| epoch   1 |   505/ 1320 batches | lr 5.00 | ms/batch 220.89 | loss  6.10 | ppl   444.73\n",
      "| epoch   1 |   506/ 1320 batches | lr 5.00 | ms/batch 192.82 | loss  5.69 | ppl   295.89\n",
      "| epoch   1 |   507/ 1320 batches | lr 5.00 | ms/batch 169.45 | loss  6.19 | ppl   487.12\n",
      "| epoch   1 |   508/ 1320 batches | lr 5.00 | ms/batch 218.53 | loss  6.11 | ppl   450.87\n",
      "| epoch   1 |   509/ 1320 batches | lr 5.00 | ms/batch 165.60 | loss  6.22 | ppl   502.15\n",
      "| epoch   1 |   510/ 1320 batches | lr 5.00 | ms/batch 215.66 | loss  6.29 | ppl   539.02\n",
      "| epoch   1 |   511/ 1320 batches | lr 5.00 | ms/batch 170.83 | loss  6.00 | ppl   402.14\n",
      "| epoch   1 |   512/ 1320 batches | lr 5.00 | ms/batch 169.50 | loss  6.05 | ppl   424.40\n",
      "| epoch   1 |   513/ 1320 batches | lr 5.00 | ms/batch 175.84 | loss  6.01 | ppl   407.88\n",
      "| epoch   1 |   514/ 1320 batches | lr 5.00 | ms/batch 199.54 | loss  6.07 | ppl   430.70\n",
      "| epoch   1 |   515/ 1320 batches | lr 5.00 | ms/batch 246.78 | loss  6.08 | ppl   435.27\n",
      "| epoch   1 |   516/ 1320 batches | lr 5.00 | ms/batch 576.05 | loss  6.19 | ppl   485.93\n",
      "| epoch   1 |   517/ 1320 batches | lr 5.00 | ms/batch 519.92 | loss  6.17 | ppl   478.72\n",
      "| epoch   1 |   518/ 1320 batches | lr 5.00 | ms/batch 379.31 | loss  6.15 | ppl   467.20\n",
      "| epoch   1 |   519/ 1320 batches | lr 5.00 | ms/batch 474.25 | loss  6.19 | ppl   486.03\n",
      "| epoch   1 |   520/ 1320 batches | lr 5.00 | ms/batch 355.61 | loss  5.99 | ppl   400.02\n",
      "| epoch   1 |   521/ 1320 batches | lr 5.00 | ms/batch 343.57 | loss  6.28 | ppl   532.64\n",
      "| epoch   1 |   522/ 1320 batches | lr 5.00 | ms/batch 299.20 | loss  6.10 | ppl   445.82\n",
      "| epoch   1 |   523/ 1320 batches | lr 5.00 | ms/batch 314.49 | loss  6.11 | ppl   451.30\n",
      "| epoch   1 |   524/ 1320 batches | lr 5.00 | ms/batch 283.89 | loss  5.91 | ppl   366.95\n",
      "| epoch   1 |   525/ 1320 batches | lr 5.00 | ms/batch 184.35 | loss  5.98 | ppl   393.61\n",
      "| epoch   1 |   526/ 1320 batches | lr 5.00 | ms/batch 236.87 | loss  6.18 | ppl   481.11\n",
      "| epoch   1 |   527/ 1320 batches | lr 5.00 | ms/batch 183.08 | loss  5.94 | ppl   379.35\n",
      "| epoch   1 |   528/ 1320 batches | lr 5.00 | ms/batch 219.96 | loss  6.09 | ppl   441.97\n",
      "| epoch   1 |   529/ 1320 batches | lr 5.00 | ms/batch 196.20 | loss  6.05 | ppl   424.04\n",
      "| epoch   1 |   530/ 1320 batches | lr 5.00 | ms/batch 190.24 | loss  6.00 | ppl   404.98\n",
      "| epoch   1 |   531/ 1320 batches | lr 5.00 | ms/batch 216.73 | loss  6.28 | ppl   531.28\n",
      "| epoch   1 |   532/ 1320 batches | lr 5.00 | ms/batch 204.75 | loss  6.33 | ppl   559.78\n",
      "| epoch   1 |   533/ 1320 batches | lr 5.00 | ms/batch 200.37 | loss  6.22 | ppl   500.48\n",
      "| epoch   1 |   534/ 1320 batches | lr 5.00 | ms/batch 186.93 | loss  6.04 | ppl   421.13\n",
      "| epoch   1 |   535/ 1320 batches | lr 5.00 | ms/batch 181.23 | loss  6.05 | ppl   423.77\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     48\u001b[0m     epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbptt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_causal_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_data, bptt, ntokens, criterion, device, use_causal_mask)\n\u001b[1;32m     51\u001b[0m     val_ppl \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(val_loss)\n",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data, bptt, criterion, ntokens, optimizer, scheduler, epoch, device, use_causal_mask)\u001b[0m\n\u001b[1;32m     23\u001b[0m     src_mask \u001b[38;5;241m=\u001b[39m generate_square_subsequent_mask(data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# print(f\"Batch {batch}:\")\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# print(f\"  Data shape: {data.shape}\")\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# print(f\"  Mask shape: {src_mask.shape if src_mask is not None else 'None'}\")\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ntokens), targets)\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 47\u001b[0m, in \u001b[0;36mTransformerModelManualAttn.forward\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# print(\"Generating mask\", src.size(0))\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     src_mask \u001b[38;5;241m=\u001b[39m generate_square_subsequent_mask(src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mto(src\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 47\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(output)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, mask: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_blocks:\n\u001b[0;32m----> 8\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 37\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, mask: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    Forward pass of the EncoderBlock.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    :param x: input tensor\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    :param mask: attention mask\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    :return: output tensor of the encoder block\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     attn_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_self_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m attn_x)\n\u001b[1;32m     40\u001b[0m     ff_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_forward(x)\n",
      "Cell \u001b[0;32mIn[4], line 51\u001b[0m, in \u001b[0;36mEncoderBlock._self_attn\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_self_attn\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, mask: Optional[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    Apply self-attention to the input tensor.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    :return: tensor after applying self-attention and dropout\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 58\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m=\u001b[39m T\n\u001b[1;32m     57\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_query(q)\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     59\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_key(v)\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Apply attention\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Load and batch data\n",
    "train_iter = PennTreebank(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "train_iter, val_iter, test_iter = PennTreebank()\n",
    "train_data = data_process(train_iter, vocab=vocab, tokenizer=tokenizer)\n",
    "val_data = data_process(val_iter, vocab=vocab, tokenizer=tokenizer)\n",
    "test_data = data_process(test_iter, vocab=vocab, tokenizer=tokenizer)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bptt = 35\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size, device=device)\n",
    "val_data = batchify(val_data, eval_batch_size, device=device)\n",
    "test_data = batchify(test_data, eval_batch_size, device=device)\n",
    "\n",
    "# Model Parameters\n",
    "ntokens = len(vocab)\n",
    "emsize = 200\n",
    "d_hid = 200\n",
    "nlayers = 2\n",
    "nhead = 2\n",
    "dropout = 0.1\n",
    "use_causal_mask = True\n",
    "\n",
    "# Create Model\n",
    "model = TransformerModelManualAttn(emsize=emsize, ntoken= ntokens, d_model= d_hid, nhead=nhead, d_hid=d_hid, nlayers=nlayers, dropout=dropout).to(device)\n",
    "### Run model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, )\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs = 10\n",
    "\n",
    "# Train\n",
    "home = os.path.join(os.path.expanduser(\"~\"), \"transformer_test\")\n",
    "save_dir = os.path.join(os.path.join(home, 'pytorch_example')) # feel free to change path\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "best_model_params_path = os.path.join(save_dir, \"best_model_params.pt\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, train_data, bptt, criterion, ntokens, optimizer, scheduler, epoch, device, use_causal_mask)\n",
    "    val_loss = evaluate(model, val_data, bptt, ntokens, criterion, device, use_causal_mask)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "    scheduler.step()\n",
    "model.load_state_dict(torch.load(best_model_params_path))  # load best model states\n",
    "\n",
    "# Test\n",
    "test_loss = evaluate(model, test_data, bptt, ntokens, criterion, device)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculating Attention Complexity\n",
    "$Q, K, V ∈ \\mathbb{R}^{n×d}$ are the query, key, and value matrices, where $n$ is the sequence length and $d$ is the hidden dimension.\n",
    "\n",
    "$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V$\n",
    "\n",
    "#### Computational Complexity calculation: \n",
    "1. *Matrix Multiplication:* $QK^T$ is a matrix multiplication of size $n×n$. --> Cost of $O(n^2d)$\n",
    "2. *Scaling:* Divide by $\\sqrt{d}$. --> Cost of $O(n^2)$\n",
    "3. *Softmax:* Compute softmax along rows. --> Cost of $O(n^2)$\n",
    "4. *Weighted Sum:* Multiply softmax scores with $V$. --> Cost of $O(n^2d)$\n",
    "\n",
    "Assuming $d$ is a constant and $d <<< n$, the total cost $O(n^2)$.\n",
    "\n",
    "#### Memory Cost\n",
    "- Is also $O(n^2)$ memory complexity\n",
    "- Since this the largest storage is for the attention matrix, whose size is $n×n$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas to Reduce Attention Costs\n",
    "\n",
    "1. **Sparsity:**\n",
    "   - Sparse Attention: $\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{(Q \\odot M)K^T}{\\sqrt{d}})V, \\quad M \\in \\{0,1\\}^{n \\times n}, |M| = O(n)$ (from Sparse Transformer like BigBird)\n",
    "   - Element wise product only has cost $O(n)$ instead of $O(n^2)$ for matrix multiplication.\n",
    "\n",
    "\n",
    "2. **Lower rank approximations:**\n",
    "   If the attention matrix is low-rank, it can be approximated using a low-rank factorization. This reduces the complexity to **O(n)**.\n",
    "   As follows $\\text{Attention}(Q, K, V) \\approx \\text{softmax}(\\frac{E^T(FV)}{\\sqrt{d}}), \\quad E,F \\in \\mathbb{R}^{k \\times n}, k \\ll n$ (from Linformer)\n",
    "\n",
    "\n",
    "3. **Memory-Efficient Attention:**\n",
    "   Similar to how DCT can be perfromed in blocks to speeden up computation, attention can also be performed in blocks.\n",
    "   \n",
    "   $\\text{Attention}(Q, K, V) = \\text{BlockSoftmax}(\\frac{QK^T}{\\sqrt{d}})V, \\quad \\text{computed in blocks } B_{ij} \\in \\mathbb{R}^{b \\times b}$ (from FlashAttention)\n",
    "\n",
    "\n",
    "4. **Hierarchical Approaches**\n",
    "   - Similar to how images can be processed at different scales and then combined across scales of the input, attention can also be processed at different heirachies. In this case, the heirachies can be processed in parallel and then combined.Thus, the complexity is reduced. The heirachies would be different levels of abstraction of the input/local-global features etc. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we used causal masking for the attention as follows : $$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d}} \\odot M)V$$\n",
    "\n",
    "Where $$M_{ij} = \\begin{cases} 0 & \\text{if } i > j \\\\ -\\infty & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "Mathematically, this ensures that the attention mechanism only looks at the past tokens and not the future tokens. This is useful for autoregressive models like LSTMs, Transformers etc. where the model should not have access to future tokens.\n",
    "However, we can have other versions of masking like:\n",
    "\n",
    "1. **Bidirectional Masking:**\n",
    "   This is the opposite of causal masking. Here, the model can look at both past and future tokens. This is useful for models like BERT where the model should have access to both past and future tokens.\n",
    "    $$M_{ij} = 1 \\quad \\forall i,j$$\n",
    "\n",
    "2. **Block Masking:**\n",
    "   Where b is the block size. This groups tokens into blocks, allowing for efficient processing of long sequences.\n",
    "   $$M_{ij} = \\begin{cases} 0 & \\text{if } \\lfloor i/b \\rfloor = \\lfloor j/b \\rfloor \\\\ -\\infty & \\text{otherwise} \\end{cases}$$\n",
    "3. **Random Masking:**\n",
    "   Randomly mask some tokens. This is useful for training robust models that can handle missing tokens.\n",
    "   $$M_{ij} = \\begin{cases} 0 & \\text{with probability } p \\\\ -\\infty & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "4. **Window Masking:**\n",
    "   This restricts attention to a local context, useful for tasks with primarily local dependencies.\n",
    "   $$M_{ij} = \\begin{cases} 0 & \\text{if } |i-j| \\leq w \\\\ -\\infty & \\text{otherwise} \\end{cases}$$\n",
    "Where w is the window size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
